== Chapter 9: MicroProfile Telemetry

This chapter focuses on distributed tracing and observability for Java-based microservices.  In a microservices architecture, where requests traverse multiple services, understanding the flow of operations across these services is crucial for monitoring performance, diagnosing issues, and optimizing system behavior. MicroProfile Telemetry enables developers to capture and visualize telemetry data, such as trace information and context propagation, ensuring transparency and enhancing the reliability of complex distributed systems. This chapter covers various topics about how to gain insights into the flow of requests and monitor the performance of service, and implement robust tracing mechanisms to optimize the behavior and reliability of microservices.

=== Topics to be covered:

* Introduction to MicroProfile Telemetry
* Tracing Concepts (Spans, Traces and Context Propagation)
* Instrumenting MicroProfile Telemetry
* Switching Tracing Providers
* Context Propagation and Correlation
* Analyzing Traces
* Security Considerations for Tracing

=== Introduction to MicroProfile Telemetry

MicroProfile Telemetry is a specification that provides a vendor-neutral API for instrumenting, collecting, and exporting telemetry data from microservices applications. Built on the OpenTelemetry project, an open-source observability framework, it offers a comprehensive set of tools and libraries for collecting and analyzing telemetry data. By integrating seamlessly with MicroProfile, it allows developers to implement distributed tracing, metrics, and logging with minimal effort.

MicroProfile Telemetry makes it easy for developers to add tracing, metrics, and logging to their microservices applications. It provides a consistent API across different vendors, so developers can easily switch between different implementations without having to rewrite their code, promoting portability and reducing vendor lock-in. The specification supports capturing detailed telemetry data, such as trace information and context propagation, enabling better monitoring and debugging of distributed systems. This enhanced visibility is critical for optimizing system behavior, ensuring reliability, and improving the overall user experience in complex microservices architectures.

=== Tracing Concepts (Spans, Traces and Context Propagation)

Tracing is a powerful tool for understanding the flow of requests through a distributed system. Tracing concepts such as *spans*, *traces*, and *context propagation* are fundamental to achieving observability and gaining insights into the system's behavior.

==== Spans

A *span* is the basic unit of work in tracing. It represents a single operation or task performed by a service, such as an HTTP request, a database query, or a computation. Each span contains metadata, including:

* *Operation Name*: Describes the activity (e.g., +HTTP GET /orders+).
* *Start Time and Duration*: Captures when the operation started and how long it took.
* *Attributes*: Key-value pairs providing context (e.g., user IDs, resource names, HTTP status codes).
* *Parent Span ID*: Indicates the parent span, forming a relationship within a trace.

Spans may also include additional data like logs and events, which help provide a detailed view of the operation's lifecycle. Spans are connected together to form a trace, which provides a visual representation of the flow of a request. This can be helpful for identifying bottlenecks and performance issues.

==== Traces

A *trace* is a collection of related spans that represent the end-to-end execution of a request or transaction. It provides a holistic view of how a single request flows through the system, including interactions between services. Traces often form a tree structure, where the root span represents the entry point (e.g., a user request), and child spans represent subsequent operations. +
For example:

* A user requests order details.
* The API gateway records the root span.
* Child spans are created as the request is forwarded to the order service, which interacts with a database.

==== Context Propagation

*Context propagation* ensures that trace metadata, such as trace IDs and span IDs, travels with requests as they move through different services and threads. This metadata allows spans created in different services to be correlated into a single trace. Context propagation is vital for connecting distributed spans and understanding their relationships. +

In Java, context propagation is often implemented using thread-local variables or libraries like MicroProfile Context Propagation. This ensures that tracing information is maintained seamlessly across thread pools, asynchronous tasks, and service boundaries.

Together, these concepts form the foundation of distributed tracing, enabling developers to monitor, analyze, and optimize the performance of their microservices effectively.

=== Instrumenting MicroProfile Telemetry

Instrumenting Open Telemetry in an E-commerce Application

*Step 1: Add the MicroProfile Telemetry Dependency*

First, include the necessary dependencies for MicroProfile Telemetry in your `pom.xml` file. These dependencies enable tracing and exporting of telemetry data.

[source, xml]
----
<dependency>
  <groupId>org.eclipse.microprofile.telemetry</groupId>
  <artifactId>microprofile-opentelemetry-api</artifactId>
  <version>2.0.1</version>
  <scope>provided</scope>
</dependency>
----

*Step 2: Create a Tracer*

A Tracer is used to create spans and manage trace data within the application. Initialize a Tracer instance for your application:

[source, java]
----
import io.opentelemetry.api.trace.Tracer; 
import io.opentelemetry.api.GlobalOpenTelemetry;

//…
//…

Tracer tracer = TracerFactory.getInstance().getTracer("my-tracer");
----

*Step 3: Create a Span*

Use the Tracer to create a span that represents a specific operation or activity in your application:

[source, java]
----
Span span = tracer.spanBuilder("my-span").startSpan();
----

*Step 4: Add Attributes to the Span*

Attributes provide additional metadata about the span, such as the HTTP method or the resource being accessed. This helps in contextualizing the trace data:

[source, java]
----
span.setAttribute(++"http.method"++, ++"+*+GET+*+"++);
span.setAttribute(++"http.url"++, ++"/products/12345"++);
span.setAttribute(++"user.id"++, ++"98765"++);+
----

*Step 5: End the Span*

When the operation completes, end the span to capture the telemetry data:

[source, java]
----
span.end();
----

*Step 6: Export the Traces*

To export traces to a backend like Jaeger, include the exporter dependency and configure the properties: +

Add the Jaeger Exporter Dependency:

[source, xml]
----
<dependency>
    <groupId>io.opentelemetry</groupId>
    <artifactId>opentelemetry-exporter-jaeger</artifactId>
    <version>1.34.1</version>
</dependency>
----

*Step 7: Configuration *

Configure the exporter in `application.properties`:

[source, properties]
----
otel.traces.exporter=jaeger
otel.exporter.jaeger.endpoint=http://localhost:14268/api/traces
----

*Step 8: Verify the Traces*

After implementing tracing, verify that the traces are being collected and exported:

. Start the Jaeger server (or your chosen backend).
. Open the Jaeger UI at http://localhost:16686[http://localhost:16686].
. Search for traces associated with your application and confirm that the telemetry data is visible.

=== Switching to Tracing Providers

MicroProfile Telemetry supports multiple tracing providers, for exporting and analyzing traces. The default tracing provider is Jaeger, but developers can also use other providers such as Zipkin or OpenCensus.

To switch to another tracing provider, replace the Jaeger dependency with the appropriate exporter dependency, such as *Zipkin*, and update the configuration properties accordingly.

=== Context Propagation and Correlation

*Context propagation* refers to the mechanism of carrying trace-related metadata, such as *trace IDs* and *span IDs*, across service and thread boundaries. This ensures that all spans created during a request can be linked together to form a complete trace.

*How it works* 

. *Trace Context*: Metadata that includes the +traceId+, +spanId+, and sampling information.
. *Propagation Mechanisms*: Trace context is typically carried in HTTP headers (e.g., +traceparent+) or message properties in message queues.
. *MicroProfile Integration*: MicroProfile Context Propagation seamlessly integrates with OpenTelemetry to ensure that the trace context is maintained across service calls.

*Example: Propagating Context Across HTTP Requests*

When making an HTTP request, the trace context is propagated using headers:

[source, java]
----
import jakarta.ws.rs.client.Client;
import jakarta.ws.rs.client.ClientBuilder;

Client client = ClientBuilder.newClient();
client.target("http://inventory-service/api/check")
      .request()
      .header("traceparent", "00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01")
      .get();
----

In this example, the +traceparent+ header ensures that the trace context is passed to the downstream +inventory-service+.

==== Correlation

*Correlation* is the process of associating related spans and traces across multiple services and threads to form a cohesive view of a transaction. Correlation enables developers to:

* Identify the source of bottlenecks or errors in distributed systems.
* Understand the dependencies and interactions between services.

==== Trace and Span IDs

* *Trace ID*: A unique identifier shared across all spans in a single trace.
* *Span ID*: A unique identifier for a single span. It is linked to a parent span, forming a hierarchy.

*Example: Correlating Logs with Traces*

By including trace and span IDs in logs, you can correlate logs with traces to gain deeper insights:

[source, java]
----
import org.slf4j.MDC;

MDC.put("traceId", "4bf92f3577b34da6a3ce929d0e0e4736");
MDC.put("spanId", "00f067aa0ba902b7");

log.info("Fetching product details for productId=12345");
----

When viewing logs, the +traceId+ and +spanId+ allow you to link specific log entries to the corresponding spans in your tracing system.

==== Context Propagation in Asynchronous Flows

In asynchronous programming, maintaining context across threads is challenging. MicroProfile Context Propagation helps by enabling trace context to be passed seamlessly across asynchronous tasks.

*Context Propagation in Async Tasks*

[source, java]
----
import org.eclipse.microprofile.context.ThreadContext;
import java.util.concurrent.CompletableFuture;

ThreadContext threadContext = ThreadContext.builder().build();

CompletableFuture.runAsync(threadContext.contextualRunnable(() -> {
    Span span = tracer.spanBuilder("async-task").startSpan();
    try {
        // Perform async operations
    } finally {
        span.end();
    }
}));
----

This ensures that the trace context is preserved, allowing the spans created in the asynchronous task to be linked correctly to the trace.

==== Best Practices for Context Propagation and Correlation

. *Propagate Context Consistently: *Use standard headers like traceparent for HTTP and custom headers for other protocols.
. *Log Trace Identifiers: *Include trace and span IDs in logs to correlate logs and traces effectively.
. *Use Context Propagation Libraries:* Leverage tools like MicroProfile Context Propagation to simplify the management of context in asynchronous flows.
. *Secure Context Data: *Ensure that trace metadata does not include sensitive information and is transmitted securely.

By leveraging context propagation and correlation, developers can gain a unified view of distributed transactions, enabling effective debugging and optimization of microservices.

=== Analyzing Traces

Once trace data is collected and exported to a backend system, analyzing these traces becomes a crucial step in understanding the behavior of your distributed microservices architecture. By examining traces, you can gain insights into system performance, identify bottlenecks, and detect failures or anomalies.

==== Steps to Analyze Traces

===== 1. Visualizing Traces

Tracing backends like *Jaeger*, *Zipkin*, or *OpenTelemetry Collector* provide visual interfaces to explore and analyze traces. These tools display traces as timelines or dependency graphs, making it easier to:

* Understand the sequence of operations.
* Identify the services and components involved in a request.
* Observe how requests propagate through the system.

*Example in Jaeger:*

* Open the Jaeger UI at +http://localhost:16686+.
* Search for traces using parameters like operation name, time range, or service.
* View a detailed breakdown of each span within the trace, including timing and attributes.

===== 2. Identifying Bottlenecks

Traces highlight spans with long durations or repeated retries, which often point to bottlenecks or inefficiencies. Pay close attention to:

* *Critical Path*: The longest path in a trace that determines the total response time.
* *Service Dependencies*: Examine how upstream and downstream services interact to find slow components.
* *Retries and Failures*: Repeated spans or high failure rates indicate problematic dependencies or transient errors.

===== 3. Diagnosing Failures

Traces provide valuable information for diagnosing failures, including:

* *Error Codes*: Look for spans with error attributes, such as `http.status_code=500`.
* *Exception Details*: Many tracing systems capture stack traces or error messages in spans.
* *Service Impact*: Identify which upstream and downstream services are affected by the failure.

===== 4. Understanding Service Dependencies

Dependency graphs generated from traces show the interactions between services. These graphs help:

* Visualize which services depend on each other.
* Detects circular dependencies or excessive coupling.
* Plan optimizations by focusing on critical services.

===== 5. Correlating Traces with Logs and Metrics

Traces, when combined with logs and metrics, provide a comprehensive picture of the system:

* *Logs*: Use trace IDs and span IDs in logs to correlate application logs with specific spans.
* *Metrics*: Correlate trace performance data with system metrics like CPU usage, memory consumption, or request rates.
Example: If a span indicates high latency, check corresponding logs and metrics to identify the underlying cause, such as a resource constraint or network delay.

==== Tools for Trace Analysis

===== Jaeger

* Provides detailed timelines for traces.
* Offers a dependency graph visualization.
* Supports searching and filtering based on trace attributes.

=====  Zipkin

* Focuses on simplicity and quick trace searches.
* Integrates with multiple programming languages and tracing libraries.

===== OpenTelemetry Collector

* Centralizes trace collection and routing to different backends.
* Supports advanced features like sampling and transformation.

==== Best Practices for Analyzing Traces

. *Establish Baselines*: Use traces to establish performance baselines for services.
. *Monitor Critical Paths*: Focus on traces that traverse critical services or user-facing operations.
. *Use Sampling Strategically*: Balance trace volume and storage costs by sampling traces intelligently.
. *Automate Alerts*: Set up alerts for abnormal patterns in traces, such as increased latency or failure rates.
. *Collaborate Across Teams*: Share trace insights with development, operations, and QA teams to improve system reliability.

By analyzing traces effectively, you can identify opportunities to optimize your microservices, ensure smoother operations, and enhance the overall user experience. Tracing tools provide a powerful way to visualize and understand the intricate dynamics of distributed systems. +
When analyzing traces, developers should look for the following:

* *Long spans:* Spans that take a long time to complete may indicate a performance issue.
* *Missing spans:* Missing spans can make it difficult to understand the flow of a request.
* *Errors:* Errors can indicate problems with a service or a request.
* *High latency:* High latency can indicate a problem with the network or a service.

By analyzing traces, developers can identify and troubleshoot problems with their microservices applications. This can help developers improve the performance and reliability of their applications.

Here are some tips for analyzing traces:

* *Use a trace viewer:* A trace viewer is a tool that can help you visualize and analyze traces.
* *Look for patterns:* Look for patterns in the traces that may indicate a problem.
* *Correlate traces with metrics:* Correlate traces with metrics to get a better understanding of the performance of your application.
* *Use sampling:* Use sampling to reduce the number of traces that are collected. This can improve the performance of your tracing system.

By following these tips, developers can effectively analyze traces to improve the performance and reliability of their microservices applications.

==== Security Considerations for Tracing

When implementing tracing in your applications, it is crucial to be mindful of security implications. Tracing involves collecting and storing data about application behavior, which can potentially expose sensitive information if not handled properly.

* *Data Sensitivity:* Be cautious about the data included in traces. Avoid logging sensitive information such as passwords, API keys, or personally identifiable information (PII).
* *Access Control:* Implement strict access controls to limit who can view and manage trace data.
* *Encryption:* Consider encrypting trace data at rest and in transit to protect it from unauthorized access.
* *Storage:* Carefully manage the storage of trace data. Avoid storing traces indefinitely and implement data retention policies.
* *Third-Party Services:* If using third-party tracing services, ensure they have robust security measures in place to protect your data.

===== 1. Avoid Capturing Sensitive Data

Traces often include attributes and metadata that can contain sensitive information. Avoid storing or transmitting sensitive details, such as:

* Personally Identifiable Information (PII) (e.g., names, addresses, social security numbers).
* Payment information (e.g., credit card numbers).
* Authentication credentials (e.g., passwords, API keys, tokens).

*Best Practice:*

Sanitize attributes before adding them to spans:

[source, java]
----
span.setAttribute("user.id", "anonymized-user-id");
span.setAttribute("credit.card.last4", "****1234");
----

===== 2. Encrypt Trace Data

To prevent unauthorized access during transmission, ensure that telemetry data is encrypted. Use secure protocols such as HTTPS or TLS for exporting trace data to a backend.
 
 *Example:*

* Configure the tracing provider to use encrypted connections:

[source, properties]
----
otel.exporter.jaeger.endpoint=https://secure-jaeger-collector.example.com
otel.exporter.otlp.endpoint=https://secure-collector.example.com
----

===== 3. Limit Trace Retention

Trace data can grow rapidly in distributed systems. Retaining it indefinitely increases the risk of exposing sensitive information. Implement retention policies to:

* Retain traces only for the necessary duration for debugging or performance analysis.
* Periodically purge older traces from storage.

===== 4. Access Control and Auditing

Restrict access to trace data to authorized personnel only. Ensure that your tracing backend implements robust authentication and authorization mechanisms.

*Best Practice:*

* Use role-based access control (RBAC) to define permissions for viewing and managing traces.
* Audit access to trace data regularly to identify potential misuse or breaches.

===== 5. Sampling Strategies to Minimize Exposure

Sampling reduces the volume of traces collected and limits the exposure of sensitive data by capturing only a subset of requests. Common strategies include:

* Random Sampling: Captures a fixed percentage of traces.
* Rate-Limiting Sampling: Limits the number of traces per second.
* Key-Based Sampling: Samples traces based on specific attributes (e.g., user ID).

*Example:*

Configure sampling to capture traces for debugging specific operations:

[source, properties]
----
otel.traces.sampler=traceidratio
otel.traces.sampler.traceidratio=0.1
----

===== 6. Compliance with Regulations

Ensure that your tracing practices comply with data protection and privacy regulations such as GDPR, CCPA, or HIPAA. Key considerations include:

* Anonymizing sensitive data before tracing.
* Informing users about telemetry collection in your privacy policy.
* Providing mechanisms to opt out of tracing where required.

===== 7. Isolate Tracing Infrastructure

The tracing infrastructure, such as Jaeger or OpenTelemetry Collector, should be isolated from the public internet and accessible only within secure networks. 

*Best Practice:*

* Deploy tracing backends in private subnets or behind firewalls.
* Use VPNs or dedicated connections for remote access to tracing dashboards.

===== 8. Monitor and Alert on Trace Anomalies

Tracing can help detect potential security incidents. Monitor traces for unusual patterns, such as:

* Unexpected spikes in requests.
* Requests from unknown or unauthorized sources.
* Abnormal response times indicating possible exploits.
Set up alerts for these anomalies to investigate and mitigate potential issues. +
By following these security considerations, you can leverage the benefits of distributed tracing without compromising the security of your system or the privacy of your users. Careful handling of trace data, coupled with robust encryption, access controls, and compliance practices, ensures that tracing remains a valuable yet secure component of your observability strategy.

=== Conclusion

MicroProfile Telemetry provides a robust foundation for observability in Java-based microservices, enabling developers to implement distributed tracing seamlessly. By leveraging this specification, you can gain deep insights into the flow of requests, identify bottlenecks, and enhance the reliability and performance of your applications. The integration of standardized tracing concepts like spans, traces, and context propagation ensures that developers can maintain a cohesive understanding of their system's behavior across service boundaries.

Through instrumentation, context propagation, and effective trace analysis, MicroProfile Telemetry simplifies the complexities of monitoring and debugging distributed systems. It empowers teams to proactively address issues, optimize performance, and improve the user experience. Moreover, by adhering to security best practices, developers can ensure that telemetry data is protected, compliant with regulations, and free of sensitive information.

In this chapter, we explored the critical security considerations surrounding tracing within the MicroProfile Telemetry framework. We emphasized the importance of safeguarding sensitive data by avoiding the inclusion of Personally Identifiable Information (PII) in trace spans. Additionally, we discussed the potential security risks associated with tracing in production environments and the significance of carefully managing sampling rates and data retention policies. By adhering to these security best practices, developers can harness the power of tracing for observability while ensuring the confidentiality and integrity of their applications.

As microservices architectures continue to evolve, the ability to observe and trace system interactions will remain a critical factor in maintaining resilient and efficient applications. MicroProfile Telemetry stands as a valuable tool in achieving these goals, providing developers with the observability they need to deliver reliable, high-performance microservices in modern cloud-native environments.
